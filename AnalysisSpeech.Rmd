---
title: "Analysis Speech"
author: "Tom Offrede"
date: "2022-09-15"
output:
  html_document:
    toc: true
    toc_float: 
      collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(lme4)
library(ggdist)

`%!in%` <- Negate(`%in%`)

folder <- "C:/Users/offredet/Documents/1HU/ExperimentEyes/Data/All/"
files <- list.files(folder, "RData")
fileBase <- files[grepl("Baseline", files)]
fileConv <- files[grepl("Conversation", files) & !grepl("Including", files)]

load(paste0(folder, fileBase))
load(paste0(folder, fileConv))

dac <- dac[!grepl("Impairment|Dyslexia|Gender|Education|Age", names(dac))]

dac <- dac %>%
  mutate_at(c("turnDur", "turn", "f0mean", "robPrevf0", "gapDur", "intimMean", "intimSD", "intimMed"), as.numeric) %>%
  mutate_at(c("speaker", "condition", "Order", "task"), as.factor) %>% 
  group_by(speaker) %>%
  mutate(turnDurNormal = (turnDur - min(turnDur, na.rm = TRUE)) / (max(turnDur, na.rm = TRUE) - min(turnDur, na.rm = TRUE)),
         turnDurc = turnDur - mean(turnDur, na.rm=TRUE),
         robf0c = robPrevf0 - mean(robPrevf0, na.rm=TRUE),
         intimc = intimMean - mean(intimMean, na.rm=TRUE)) %>%
  ungroup() %>% 
  mutate(ESc = EmotionalStability - mean(EmotionalStability, na.rm=TRUE),
         Agreec = Agreeableness - mean(Agreeableness, na.rm=TRUE),
         Conscc = Conscientiousness - mean(Conscientiousness, na.rm=TRUE),
         Extrac = Extraversion - mean(Extraversion, na.rm=TRUE),
         IOc = IntellectOpenness - mean(IntellectOpenness, na.rm=TRUE)) # %>% 
  # filter(scoreEN > 60) # see with and without low-level participants

dab <- dab %>%
  mutate_at(c("condition"), as.factor)
```

## Conditions:
**GA: Gaze Aversion; the robot produced gaze aversion. Experimental condition.**
**NG: No Gaze aversion; the robot stared constantly at the human. Control condition.**

# Between-Turn Gap Duration

```{r}
ggplot(dac, aes(condition, gapDur))+
  geom_boxplot()

ggplot(dac, aes(turnNormal, gapDur))+
  geom_point()+
  facet_wrap(~condition)+
  geom_smooth(method="loess")
```

* There is a `turn` effect: longer gaps throughout the interaction. This model was better than the one using `intimacy` values
* There is **no `condition` effect**


```{r}
dac$condition <- relevel(dac$condition, ref="NG")
summary(g1 <- lmer(gapDur ~ condition * turnNormal + (1 + condition | speaker), dac))
summary(g2 <- lmer(gapDur ~ condition * intimc + (1 + condition | speaker), dac))
anova(g1, g2)
```

# Turn Duration

```{r}
ggplot(dac %>% filter(!grepl("Robot", speaker)), aes(condition, turnDur))+
  geom_boxplot()

ggplot(dac %>% filter(!grepl("Robot", speaker)), aes(turnNormal, turnDur))+
  geom_point()+
  facet_wrap(~condition)+
  geom_smooth(method="loess")
```

* There is a `turn` effect: the further into the conversation, the longer the turns
* There is a `turn : condition` interaction: in the GA condition (robot looks away; i.e. objectively more humanlike) the turns get longer throughout the conversation

The order of the `turn` has a .95 correlation with `intimacy` ratings, since the questions always followed the same order. So we can't say if turn duration got higher with the progression of the conversation or because the questions got more intimate, or simply because the questions got more elaborate (so people had more things to talk about).

So if we make a model with `turn` or `intimacy`, both give the same results -- the one with `turn` showing slightly larger effect sizes. The model with `turn` also has considerably lower AIC.

```{r}
cor.test(dac$turnNormal, dac$intimMean)

dac$condition <- relevel(dac$condition, ref="NG")
summary(t1 <- lmer(turnDur ~ condition * turnNormal + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))

summary(t2 <- lmer(turnDur ~ condition * intimMean + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))

anova(t1, t2)
```

# f0 during conversation

## Look closer at f0 throughout time

(Check plots saved)

```{r}
# folderRobF0 <- "C:/Users/offredet/Documents/1HU/ExperimentEyes/Data/RobotsF0/"
# dat <- dac %>% filter(grepl("Robot", speaker))
# 
# for(r in unique(dat$speaker)){
#   ggplot(dat %>% filter(speaker==r), aes(as.numeric(timeIndexOverall), f0mean))+
#     geom_point()+
#     facet_wrap(~condition)+
#     geom_smooth(method="loess")
#   ggsave(paste0(folderRobF0, r, ".png"))
# }
# 
# folderPartF0 <- "C:/Users/offredet/Documents/1HU/ExperimentEyes/Data/ParticipantsF0/"
# dat <- dac %>% filter(!grepl("Robot", speaker))
# 
# for(r in unique(dat$speaker)){
#   ggplot(dat %>% filter(speaker==r), aes(as.numeric(timeIndexOverall), f0mean))+
#     geom_point()+
#     facet_wrap(~condition)+
#     geom_smooth(method="loess")
#   ggsave(paste0(folderPartF0, r, ".png"))
# }
```


## Using the f0 from the IPUs of the robot's previous turn as predictors for the f0 of the IPUs of the human's current turn

```{r}
ggplot(dac, aes(robf0c, f0mean))+
  geom_point()+
  facet_wrap(~condition)+
  geom_smooth(method="lm")
```

There is an effect of `robot's f0` in interaction with `condition`.

* `condition : robot's f0`: the robot's f0 predicts the human's f0 only in the staring condition. 

See:

```{r}
dac$condition <- relevel(dac$condition, ref="NG")

summary(f1 <- lmer(f0mean ~ robf0c : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))

```

### Intimacy

```{r}
ggplot(dac, aes(intimMean, f0mean))+
  geom_point()+
  facet_wrap(~condition)+
  geom_smooth(method="lm")
```


* `intimacy`: f0 gets lower with higher intimacy.

```{r}
summary(f1 <- lmer(f0mean ~ intimc + (1  | speaker), dac %>% filter(!grepl("Robot", speaker))))
```


### BFI

For each BFI dimension, I compared a model `f0 ~ robot's f0 : condition : [BFI dimension]` to one without the dimension. Many of the models with the BFI dimension show a t > 2 for the effect of `robot's f0` and `BFI` in the NG condition (but not GA condition). But since I wouldn't expect that all BFI dimensions would predict convergence, I am comparing the AICs of the models with and without BFI. When the AIC of the model *with* BFI is lower (even with the added variable), then we can maybe assume that the BFI effect is meaningful.

**Conclusion** about BFI dimesions:

Maybe emotional stability (I think also called neuroticism) is the only potentially good predictor.

#### Intellect/Openness

AIC of model with BFI dimension is higher.

```{r}
summary(intel1 <- lmer(f0mean ~ robf0c : IOc : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))

summary(no1 <- lmer(f0mean ~ robf0c : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))
anova(intel1, no1)
```

#### Agreeableness

AIC is the same in both models.

```{r}
d <- dac %>%
  filter(!grepl("Robot", speaker)) %>% 
  select(f0mean, robf0c, Agreec, Agreeableness, condition, speaker) %>% 
  na.omit()

summary(agree1 <- lmer(f0mean ~ robf0c : Agreec : condition + (1 | speaker), d))

summary(no1 <- lmer(f0mean ~ robf0c : condition + (1 | speaker), d))
anova(agree1, no1)
```
#### Conscientiousness

same AIC

```{r}
d <- dac %>%
  filter(!grepl("Robot", speaker)) %>% 
  select(f0mean, robf0c, Conscc, condition, speaker) %>% 
  na.omit()

summary(con1 <- lmer(f0mean ~ robf0c : Conscc : condition + (1 + condition | speaker), d))

summary(no1 <- lmer(f0mean ~ robf0c : condition + (1 + condition | speaker), d))
anova(con1, no1)
```

#### EmotionalStability

AIC of model with `emotional stability` is lower, and t value is higher.
(I have tried adding intimacy ratings in the interaction, but it makes the model a lot worse (AIC and t value). This makes sense since the interaction gets more and more complex.)

```{r}
d <- dac %>%
  filter(!grepl("Robot", speaker)) %>% 
  select(f0mean, robf0c, robPrevf0, ESc, EmotionalStability, condition, speaker, intimc) %>% 
  na.omit()

summary(em1c <- lmer(f0mean ~ robf0c : ESc : condition + (1 + condition | speaker), d))
summary(em1 <- lmer(f0mean ~ robf0c : EmotionalStability : condition + (1 + condition | speaker), d))


summary(no1 <- lmer(f0mean ~ robf0c : condition + (1 + condition | speaker), d))
anova(em1c, no1)


summary(em1 <- lmer(f0mean ~ robf0c : EmotionalStability : condition + (1 + condition | speaker), d))

summary(no1 <- lmer(f0mean ~ robf0c : condition + (1 + condition | speaker), d))
anova(em1, no1)

```

#### Extraversion

AIC higher

```{r}
summary(ex1 <- lmer(f0mean ~ robf0c : Extrac : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))

summary(no1 <- lmer(f0mean ~ robf0c : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))
anova(ex1, no1)
```



(Find a way to visualize the relationship of f0mean ~ robf0c * [BFI] * condition. e.g. heat map with facet_wrap?)
```{r}
# ggplot(dac, aes(IOc, robf0c))+
#   geom_point()+
#   facet_wrap(~condition)+
#   geom_smooth(method = "lm")+
#   ggtitle("IntellectOpenness")
# 
# ggplot(dac, aes(Extrac, robf0c))+
#   geom_point()+
#   facet_wrap(~condition)+
#   geom_smooth(method = "lm")+
#   ggtitle("Extraversion")
# 
# ggplot(dac, aes(ESc, robf0c))+
#   geom_point()+
#   facet_wrap(~condition)+
#   geom_smooth(method = "lm")+
#   ggtitle("Emotional Stability")
# 
# ggplot(dac, aes(Agreec, robf0c))+
#   geom_point()+
#   facet_wrap(~condition)+
#   geom_smooth(method = "lm")+
#   ggtitle("Agreeableness")
# 
# d <- as.matrix(dac %>% select(Conscc, f0mean, robf0c))
# wireframe(f0mean ~ Conscc * robf0c, dac %>% filter)
# 
# ggplot(dac, aes(robf0c, f0mean))+
#   geom_contour()+
#   # facet_wrap(~condition)+
#   # geom_smooth(method = "lm")+
#   ggtitle("Conscientiousness")
```


### Questionnaire dimensions

Same procedure as for the BFI dimensions. **THESE MODELS CONFLATE BOTH CONDITIONS**

These 2 dimensions were obtained through a PCA of the questionnaire ratings. I am calling them *Conversational Quality* and *Robot's Quality*. These are the items making up each dimension:

**Conversational quality**:

* My conversation with the robot flowed well.
* I was able to understand when the robot wanted me to speak.
* I was able to understand when the robot wanted to keep speaking.
* I enjoyed talking with the robot.
* I felt positively about the robot.
* I felt positively about the conversation.
* I felt comfortable while talking with the robot.

**Robot's quality**:

* The robot responded to me at the appropriate time.
* The robot’s face was very human-like.
* The robot’s voice was very human-like.
* The robot’s behavior was very human-like.

**Conclusion** of questionnaire dimensions: There is a tendency for *conversational quality* and *robot's quality* to predict convergence. See details for each dimension below.

#### Principal Component: Conversation Quality

AIC is slightly lower and t value higher. People that rated the conversation as better quality (see above) seemed to converge to the robot more.

```{r}
summary(cf1 <- lmer(f0mean ~ robf0c : PCConvQuality + (1 | speaker), dac %>% filter(!grepl("Robot", speaker))))

summary(no1 <- lmer(f0mean ~ robf0c + (1 | speaker), dac %>% filter(!grepl("Robot", speaker))))
anova(cf1, no1)
```

#### Principal Component: Robot's Quality

AIC is slightly lower and t value higher. So there seems to be a tendency for people to converge more to the robot they rated as higher quality.

```{r}
summary(hl1 <- lmer(f0mean ~ robf0c : PCRobotQuality + (1  | speaker), dac %>% filter(!grepl("Robot", speaker))))

summary(no1 <- lmer(f0mean ~ robf0c + (1 | speaker), dac %>% filter(!grepl("Robot", speaker))))
anova(hl1, no1)
```


### Checking with mock f0 data

Now check how robust this is with `robPreviousf0Mock`:

Here no effect shows up for `robot's MOCK f0`, so we can probably take the finding above as (somewhat?) robust.

```{r}
dac$condition <- relevel(dac$condition, ref="GA")
summary(f2 <- lmer(f0mean ~ intimMean + robPrevf0Mock : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))
```


## Using the difference between the human's mean f0 in the current turn and the robot's mean f0 in the preceding turn

```{r, include=FALSE}
dat <- dac %>% 
  filter(!duplicated(tgroup))
```


```{r}
ggplot(dat, aes(condition, f0Diff))+
  geom_boxplot()

ggplot(dat, aes(turnNormal, f0Diff))+
  geom_point()+
  facet_wrap(~condition)+
  geom_smooth(method="lm")
```

Using this method we don't see any effect on f0.

```{r}
summary(ft1 <- lmer(f0Diff ~ condition * turnNormal + (1 + condition | speaker), dat))
```

# f0 during baseline

```{r}
ggplot(dab, aes(robPrevf0, f0mean))+
  geom_point()+
  facet_wrap(~condition)+
  geom_smooth(method="loess")
```

There is no lasting convergence effect.

```{r}
dab$condition <- relevel(dab$condition, ref="GA")
summary(fb2 <- lmer(f0mean ~ robPrevf0 : condition + (1 | speaker), dab))
```

Similar result with robot's *mock* f0.

```{r}
summary(fbM <- lmer(f0mean ~ robPrevf0Mock : condition + (1 | speaker), dab))
```


# Questionnaire ratings and condition


```{r}
ggplot(dac, aes(condition, PCConvQuality))+
  stat_halfeye(adjust = .5,  width = .5, justification = -.2, .width = c(.5, .95), fill="#9bafbd")+
  geom_boxplot(width=.13, notch = TRUE)

ggplot(dac, aes(condition, PCConvQuality))+
  geom_boxplot()
```

From the graphs, you'd think there's either no difference between conditions or that `GA` has slightly higher ratings. The regression is confusing about it:

* If you include a random slope of `condition` per `speaker`, the model doesn't converge
* If you only include the random intercept for `speaker`, the condition `NG` is shown to have higher ratings.

I doubt the robustness of this result.

(Model's residuals: homoskedastic (good) but far from normally distributed.)

```{r}
summary(cq1 <- lmer(PCConvQuality ~ condition + (1|speaker), dac))

par(mfrow=c(2,2))
plot(fitted(cq1), resid(cq1))
hist(resid(cq1))
qqnorm(resid(cq1));qqline(resid(cq1))
```


```{r}
ggplot(dac, aes(condition, PCRobotQuality))+
  stat_halfeye(adjust = .5,  width = .5, justification = -.2, .width = c(.5, .95), fill="#9bafbd")+
  geom_boxplot(width=.13, notch = TRUE)
```

For the "robot's quality" ratings, the `NG` condition was rated higher -- opposite from our hypothesis.

(Model's residuals: also homoskedastic (good), and distribution isn't very normal but better than the conversational quality model above.)

```{r}
summary(rq1 <- lmer(PCRobotQuality ~ condition + (1|speaker), dac))

par(mfrow=c(2,2))
plot(fitted(rq1), resid(rq1))
hist(resid(rq1))
qqnorm(resid(rq1));qqline(resid(rq1))
```


