---
title: "Analysis Speech"
author: "Tom Offrede"
date: "2022-09-15"
output:
  html_document:
    toc: true
    toc_float: 
      collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(lme4)
library(ggdist)

`%!in%` <- Negate(`%in%`)

folder <- "C:/Users/offredet/Documents/1HU/ExperimentEyes/Data/All/"
files <- list.files(folder, "RData")
fileBase <- files[grepl("Baseline", files)]
fileConv <- files[grepl("Conversation", files) & !grepl("Including", files)]

load(paste0(folder, fileBase))
load(paste0(folder, fileConv))

dac <- dac[!grepl("Impairment|Dyslexia|Gender|Education|Age", names(dac))]

dac <- dac %>%
  mutate_at(c("turnDur", "turn", "f0mean", "robPrevf0", "gapDur", "intimMean", "intimSD", "intimMed"), as.numeric) %>%
  mutate_at(c("speaker", "condition", "Order", "task"), as.factor) %>% 
  group_by(speaker) %>%
  mutate(turnDurNormal = (turnDur - min(turnDur, na.rm = TRUE)) / (max(turnDur, na.rm = TRUE) - min(turnDur, na.rm = TRUE)),
         turnDurZ = (turnDur - mean(turnDur, na.rm=TRUE)) / sd(turnDur, na.rm=TRUE),
         robf0Z = (robPrevf0 - mean(robPrevf0, na.rm=TRUE)) / sd(robPrevf0, na.rm=TRUE),
         intimZ = (intimMean - mean(intimMean, na.rm=TRUE)) / sd(intimMean, na.rm=TRUE)
         ) %>%
  ungroup()

dab <- dab %>%
  mutate_at(c("condition"), as.factor)
```

## Conditions:
**GA: Gaze Aversion; the robot produced gaze aversion. Experimental condition.**
**NG: No Gaze aversion; the robot stared constantly at the human. Control condition.**

# Between-Turn Gap Duration

```{r}
ggplot(dac, aes(condition, gapDur))+
  geom_boxplot()

ggplot(dac, aes(turnNormal, gapDur))+
  geom_point()+
  facet_wrap(~condition)+
  geom_smooth(method="loess")
```

* There is a `turn` effect: longer gaps throughout the interaction. This model was better than the one using `intimacy` values
* There is **no `condition` effect**


```{r}
dac$condition <- relevel(dac$condition, ref="NG")
summary(g1 <- lmer(gapDur ~ condition * turnNormal + (1 + condition | speaker), dac))
summary(g2 <- lmer(gapDur ~ condition * intimZ + (1 + condition | speaker), dac))
anova(g1, g2)
```

# Turn Duration

```{r}
ggplot(dac %>% filter(!grepl("Robot", speaker)), aes(condition, turnDur))+
  geom_boxplot()

ggplot(dac %>% filter(!grepl("Robot", speaker)), aes(turnNormal, turnDur))+
  geom_point()+
  facet_wrap(~condition)+
  geom_smooth(method="loess")
```

* There is a `turn` effect: the further into the conversation, the longer the turns
* There is a `turn : condition` interaction: in the GA condition (robot looks away; i.e. objectively more humanlike) the turns get longer throughout the conversation

The order of the `turn` has a .95 correlation with `intimacy` ratings, since the questions always followed the same order. So we can't say if turn duration got higher with the progression of the conversation or because the questions got more intimate, or simply because the questions got more elaborate (so people had more things to talk about).

So if we make a model with `turn` or `intimacy`, both give the same results -- the one with `turn` showing slightly larger effect sizes. The model with `turn` also has considerably lower AIC.

```{r}
cor.test(dac$turnNormal, dac$intimMean)

dac$condition <- relevel(dac$condition, ref="NG")
summary(t1 <- lmer(turnDur ~ condition * turnNormal + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))

summary(t2 <- lmer(turnDur ~ condition * intimMean + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))

anova(t1, t2)
```

# f0 during conversation

## Look closer at f0 throughout time

```{r}
folderRobF0 <- "C:/Users/offredet/Documents/1HU/ExperimentEyes/Data/RobotsF0/"
dat <- dac %>% filter(grepl("Robot", speaker))

for(r in unique(dat$speaker)){
  ggplot(dat %>% filter(speaker==r), aes(as.numeric(timeIndexOverall), f0mean))+
    geom_point()+
    facet_wrap(~condition)+
    geom_smooth(method="loess")
  ggsave(paste0(folderRobF0, r, ".png"))
}

folderPartF0 <- "C:/Users/offredet/Documents/1HU/ExperimentEyes/Data/ParticipantsF0/"
dat <- dac %>% filter(!grepl("Robot", speaker))

for(r in unique(dat$speaker)){
  ggplot(dat %>% filter(speaker==r), aes(as.numeric(timeIndexOverall), f0mean))+
    geom_point()+
    facet_wrap(~condition)+
    geom_smooth(method="loess")
  ggsave(paste0(folderPartF0, r, ".png"))
}
```



## Using the f0 from the IPUs of the robot's previous turn as predictors for the f0 of the IPUs of the human's current turn

```{r}
ggplot(dac, aes(robf0Z, f0mean))+
  geom_point()+
  facet_wrap(~condition)+
  geom_smooth(method="lm")

ggplot(dac, aes(intimMean, f0mean))+
  geom_point()+
  facet_wrap(~condition)+
  geom_smooth(method="lm")
```

There seem to be effects of `intimacy` and `robot's f0` in interaction with `condition`. However, I am not sure what model to use.

If we make a model only with (`interaction of intimacy and condition` + `interaction of robot's f0 and condition`), the only |t| > 2 is:

* `no-GA condition : robot's f0`: the robot's f0 predicts the human's f0 only in the no-gaze aversion condition). See:

```{r}
dac$condition <- relevel(dac$condition, ref="NG")

summary(f1 <- lmer(f0mean ~ intimZ : condition + robf0Z : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))
```

If we make a model including all the main effects plus two-way interactions between the three variables, we get:

* `intimacy : robot's f0`: positive effect of this interaction (so for a given value of intimacy, human's f0 gets higher for higher robot's f0?)
* `condition : robot's f0`: robot's f0 positively predicts human's f0 only in NG condition

```{r}
summary(f1 <- lmer(f0mean ~ (intimZ + condition + robf0Z)^2 + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))
```

If we make a model with the interaction of `robot's f0` and `condition` and a main effect of `intimacy`, we get:

* `robot's f0 : condition`: robot'S f0 predicts human's f0 only in the staring condition (i.e., the finding we had already reported in the P&P paper)
* (Intimacy tends to reduce f0, but this doesn't reach significance)

This seems the easiest model to explain, and with nicer results to talk about. But is this p-hacking?

```{r}
summary(f1 <- lmer(f0mean ~ intimZ + robf0Z : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))
```


### BFI

For each BFI dimension, I compared a model `f0 ~ robot's f0 : condition : [BFI dimension]` to one without the dimension. Many of the models with the BFI dimension show a t > 2 for the effect of `robot's f0` and `BFI` in the NG condition (but not GA condition). But since I wouldn't expect that all BFI dimensions would predict convergence, I am comparing the AICs of the models with and without BFI. When the AIC of the model *with* BFI is lower (even with the added variable), then we can maybe assume that the BFI effect is meaningful.

**Conclusion** about BFI dimesions:

Maybe emotional stability (I think also called neuroticism) is the only potentially good predictor.

#### Intellect/Openness

AIC of model with BFI dimension is not lower.

```{r}
summary(intel1 <- lmer(f0mean ~ robf0Z : IntellectOpenness : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))

summary(no1 <- lmer(f0mean ~ robf0Z : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))
anova(intel1, no1)
```

#### Agreeableness

AIC is the same in both models

```{r}
d <- dac %>%
  filter(!grepl("Robot", speaker)) %>% 
  select(f0mean, robf0Z, Agreeableness, condition, speaker) %>% 
  na.omit()

summary(agree1 <- lmer(f0mean ~ robf0Z : Agreeableness : condition + (1 + condition | speaker), d))

summary(no1 <- lmer(f0mean ~ robf0Z : condition + (1 + condition | speaker), d))
anova(agree1, no1)
```
#### Conscientiousness

same AIC

```{r}
d <- dac %>%
  filter(!grepl("Robot", speaker)) %>% 
  select(f0mean, robf0Z, Conscientiousness, condition, speaker) %>% 
  na.omit()

summary(con1 <- lmer(f0mean ~ robf0Z : Conscientiousness : condition + (1 + condition | speaker), d))

summary(no1 <- lmer(f0mean ~ robf0Z : condition + (1 + condition | speaker), d))
anova(con1, no1)
```

#### EmotionalStability

same AIC, but slightly lower BIC and lower t value in model with the dimension

```{r}
d <- dac %>%
  filter(!grepl("Robot", speaker)) %>% 
  select(f0mean, robf0Z, EmotionalStability, condition, speaker) %>% 
  na.omit()

summary(em1 <- lmer(f0mean ~ robf0Z : EmotionalStability : condition + (1 + condition | speaker), d))

summary(no1 <- lmer(f0mean ~ robf0Z : condition + (1 + condition | speaker), d))
anova(em1, no1)
```

#### Extraversion

AIC higher, and t value < 2 in model with dimension

```{r}
summary(ex1 <- lmer(f0mean ~ robf0Z : Extraversion : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))

summary(no1 <- lmer(f0mean ~ robf0Z : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))
anova(ex1, no1)
```


(Find a way to visualize the relationship of f0mean ~ robf0Z * [BFI] * condition. e.g. heat map with facet_wrap?)
```{r}
# ggplot(dac, aes(IntellectOpenness, robf0Z))+
#   geom_point()+
#   facet_wrap(~condition)+
#   geom_smooth(method = "lm")+
#   ggtitle("IntellectOpenness")
# 
# ggplot(dac, aes(Extraversion, robf0Z))+
#   geom_point()+
#   facet_wrap(~condition)+
#   geom_smooth(method = "lm")+
#   ggtitle("Extraversion")
# 
# ggplot(dac, aes(EmotionalStability, robf0Z))+
#   geom_point()+
#   facet_wrap(~condition)+
#   geom_smooth(method = "lm")+
#   ggtitle("EmotionalStability")
# 
# ggplot(dac, aes(Agreeableness, robf0Z))+
#   geom_point()+
#   facet_wrap(~condition)+
#   geom_smooth(method = "lm")+
#   ggtitle("Agreeableness")
# 
# d <- as.matrix(dac %>% select(Conscientiousness, f0mean, robf0Z))
# wireframe(f0mean ~ Conscientiousness * robf0Z, dac %>% filter)
# 
# ggplot(dac, aes(robf0Z, f0mean))+
#   geom_contour()+
#   # facet_wrap(~condition)+
#   # geom_smooth(method = "lm")+
#   ggtitle("Conscientiousness")
```


### BFI without condition

**Conclusion** about BFI dimesions (without `condition` included in the models):

Potential dimensions: `Intellect/Openness`, `Agreeableness`, `Extraversion`

#### Intellect/Openness

same AIC, though t value in model with the dimension is slightly higher

```{r}
summary(intel1 <- lmer(f0mean ~ robf0Z : IntellectOpenness + (1 | speaker), dac %>% filter(!grepl("Robot", speaker))))

summary(no1 <- lmer(f0mean ~ robf0Z + (1 | speaker), dac %>% filter(!grepl("Robot", speaker))))
anova(intel1, no1)
```

#### Agreeableness

AIC slightly lower and t value slightly higher in model with the dimension

```{r}
d <- dac %>%
  filter(!grepl("Robot", speaker)) %>% 
  select(f0mean, robf0Z, Agreeableness, condition, speaker) %>% 
  na.omit()

summary(agree1 <- lmer(f0mean ~ robf0Z : Agreeableness + (1 | speaker), d))

summary(no1 <- lmer(f0mean ~ robf0Z + (1 | speaker), d))
anova(agree1, no1)
```

#### Conscientiousness

AIC slightly lower and t value slightly higher in model with the dimension

```{r}
d <- dac %>%
  filter(!grepl("Robot", speaker)) %>% 
  select(f0mean, robf0Z, Conscientiousness, condition, speaker) %>% 
  na.omit()

summary(con1 <- lmer(f0mean ~ robf0Z : Conscientiousness + (1 | speaker), d))

summary(no1 <- lmer(f0mean ~ robf0Z + (1 | speaker), d))
anova(con1, no1)
```

#### EmotionalStability

AIC slightly higher and t value lower

```{r}
d <- dac %>%
  filter(!grepl("Robot", speaker)) %>% 
  select(f0mean, robf0Z, EmotionalStability, condition, speaker) %>% 
  na.omit()

summary(em1 <- lmer(f0mean ~ robf0Z : EmotionalStability + (1 | speaker), d))

summary(no1 <- lmer(f0mean ~ robf0Z + (1 | speaker), d))
anova(em1, no1)
```

#### Extraversion

AIC slightly lower and t value higher in model with the dimension

```{r}
summary(ex1 <- lmer(f0mean ~ robf0Z : Extraversion + (1 | speaker), dac %>% filter(!grepl("Robot", speaker))))

summary(no1 <- lmer(f0mean ~ robf0Z  + (1 | speaker), dac %>% filter(!grepl("Robot", speaker))))
anova(ex1, no1)
```

### Questionnaire dimensions

same procedure as for the BFI dimensions.

**Conclusion** of questionnaire dimensions: None of the dimensions predicts convergence. See details for each dimension below.

#### Principal Component: Conversation Quality

People that rated the conversation higher on quality (my name for this component) converged more on f0.

```{r}
summary(cf1 <- lmer(f0mean ~ robf0Z : PCConvQuality + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))

summary(no1 <- lmer(f0mean ~ robf0Z + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))
anova(cf1, no1)
```

#### Principal Component: Robot's Quality

Also, people that rated the robot higher on quality (my name for this component) converged more on f0.

```{r}
summary(hl1 <- lmer(f0mean ~ robf0Z : PCRobotQuality + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))

summary(no1 <- lmer(f0mean ~ robf0Z + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))
anova(hl1, no1)
```


### Checking with mock f0 data

Now check how robust this is with `robPreviousf0Mock`:

Here no effect shows up for `robot's MOCK f0`, so we can probably take the finding above as (somewhat?) robust.

```{r}
dac$condition <- relevel(dac$condition, ref="GA")
summary(f2 <- lmer(f0mean ~ intimMean + robPrevf0Mock : condition + (1 + condition | speaker), dac %>% filter(!grepl("Robot", speaker))))
```


## Using the difference between the human's mean f0 in the current turn and the robot's mean f0 in the preceding turn

```{r, include=FALSE}
dat <- dac %>% 
  filter(!duplicated(tgroup))
```


```{r}
ggplot(dat, aes(condition, f0Diff))+
  geom_boxplot()

ggplot(dat, aes(turnNormal, f0Diff))+
  geom_point()+
  facet_wrap(~condition)+
  geom_smooth(method="lm")
```

Using this method we don't see any effect on f0.

```{r}
summary(ft1 <- lmer(f0Diff ~ condition * turnNormal + (1 + condition | speaker), dat))
```

# f0 during baseline

```{r}
ggplot(dab, aes(robPrevf0, f0mean))+
  geom_point()+
  facet_wrap(~condition)+
  geom_smooth(method="loess")
```

There is no lasting convergence effect.

```{r}
dab$condition <- relevel(dab$condition, ref="GA")
summary(fb2 <- lmer(f0mean ~ robPrevf0 : condition + (1 | speaker), dab))
```

Similar result with robot's *mock* f0.

```{r}
summary(fbM <- lmer(f0mean ~ robPrevf0Mock : condition + (1 | speaker), dab))
```


# Questionnaire ratings and condition


```{r}
ggplot(dac, aes(condition, PCConvQuality))+
  stat_halfeye(adjust = .5,  width = .5, justification = -.2, .width = c(.5, .95), fill="#9bafbd")+
  geom_boxplot(width=.13, notch = TRUE)

ggplot(dac, aes(condition, PCConvQuality))+
  geom_boxplot()
```

From the graphs, you'd think there's even no difference between conditions or that `GA` has slightly higher ratings. The regression is being confusing about it:

* If you include a random slope of `condition` per `speaker`, the model doesn't converge
* If you only include the random intercept for `speaker`, the condition `NG` seems to have quite higher ratings.

(Model's residuals: homoskedastic (good) but far from normally distributed.)

```{r}
summary(cq1 <- lmer(PCConvQuality ~ condition + (1|speaker), dac))

par(mfrow=c(2,2))
plot(fitted(cq1), resid(cq1))
hist(resid(cq1))
qqnorm(resid(cq1));qqline(resid(cq1))
```


```{r}
ggplot(dac, aes(condition, PCRobotQuality))+
  stat_halfeye(adjust = .5,  width = .5, justification = -.2, .width = c(.5, .95), fill="#9bafbd")+
  geom_boxplot(width=.13, notch = TRUE)
```

For the "robot's quality" ratings, the `NG` condition was rated higher -- opposite from our hypothesis.

(Model's residuals: also homoskedastic (good), and distribution isn't very normal but better than the conversational quality model above.)

```{r}
summary(rq1 <- lmer(PCRobotQuality ~ condition + (1|speaker), dac))

par(mfrow=c(2,2))
plot(fitted(rq1), resid(rq1))
hist(resid(rq1))
qqnorm(resid(rq1));qqline(resid(rq1))
```


